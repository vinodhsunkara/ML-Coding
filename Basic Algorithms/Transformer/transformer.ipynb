{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5c4e4a3-ca64-46cc-9079-89c474ad704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -m pip install torch torchvision torchaudio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from math import sqrt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1389a1dd-48e9-4ba7-a142-34f6e4fd25ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer (\"bert-base-uncased\" = lowercase, 12-layer medium encoder)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "sentence_1 = \"Artificial Intelligence is a game changer.\"\n",
    "sentence_2 = \"Join the AI Career Launchpad program.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f90bbcc-ba0e-4dc7-b982-81236e9f85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    sentence_1,\n",
    "    sentence_2,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81e55eaa-8982-43c4-b353-8888d8b87ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 7976,  4454,  2003,  1037,  2208,  2689,  2099,  1012,  3693,  1996,\n",
      "          9932,  2476,  4888, 15455,  2565,  1012]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "13c249c0-0909-4496-96c1-e99275bc0406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 7976,  4454,  2003,  1037,  2208,  2689,  2099,  1012,  3693,  1996,\n",
      "          9932,  2476,  4888, 15455,  2565,  1012]])\n",
      "Tokens: ['artificial', 'intelligence', 'is', 'a', 'game', 'change', '##r', '.', 'join', 'the', 'ai', 'career', 'launch', '##pad', 'program', '.']\n"
     ]
    }
   ],
   "source": [
    "# Understand the tokens better\n",
    "print(\"Input IDs:\", inputs[\"input_ids\"])\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e536e7fe-a9e3-429c-a7e1-8235e9f8ca7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7258927-9130-41b5-91d0-673eb5f425d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(30522, 768)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a84deb68-7d8c-4e4b-8358-c132cf2cf4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "input_embeds = token_embeddings(inputs.input_ids)\n",
    "print(input_embeds.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9aac985d-35ef-4f3e-8c6d-171408c5af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "\n",
    "query = key = value = input_embeds\n",
    "\n",
    "# Attention Score Formula: Softmax[Q.K/sqrt(dim_k)].V\n",
    "def scaled_dot_product_attention(query, key, value):\n",
    "    # For scaling the Query<>Key dot product to ensure softmax doesn't have to deal with large values\n",
    "    # Last dimension in the query tensor (hidden layer dim)\n",
    "    dim_k = query.size(-1)\n",
    "    # Batch matrix multiplication (dot product), so for each item in the batch it multiplies query with key\n",
    "    scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\n",
    "\n",
    "    # We apply a softmax along the last dimension (dim=-1), so for each query position, \n",
    "    # all scores over the keys are turned into probabilities that sum to 1\n",
    "    weights = F.softmax(scores, dim = -1)\n",
    "    return torch.bmm(weights, value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ad7bb639-bae1-4df9-ac10-01233f54eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        attention_outputs = scaled_dot_product_attention(self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n",
    "        return attention_outputs\n",
    "        \n",
    "    # hidden_state tensor is of shape [batch_size, seq_len, embed_dim]\n",
    "    # nn.Linear(embed_dim, head_dim), projects each token embedding vector from embed_dim to head_dim dimension\n",
    "    # Though we pass 3D hidden_state tensor, PyTorch applies linear transformation to the last dimension only, treating the first dimensions as batch-like.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fdc92218-d648-4638-a002-3bac4249a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            embed_dim = config.hidden_size\n",
    "            num_heads = config.num_attention_heads\n",
    "            head_dim = embed_dim // num_heads\n",
    "            self.heads = nn.ModuleList([AttentionHead(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "            # final linear transformation on the concatenated output for interactions across representations from each head\n",
    "            self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        def forward(self, hidden_state):\n",
    "            # It runs the input through each attention head, collects all head outputs, \n",
    "            # and concatenates them along the last dimension to form one combined multi-head representation.\n",
    "            x = torch.cat([h(hidden_state) for h in self.heads], dim = -1)\n",
    "            x = self.output_linear(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "501475a7-b8ef-4732-8297-8457b1aa01d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "multihead_attention = MultiHeadAttention(config)\n",
    "attention_output = multihead_attention(input_embeds)\n",
    "print(attention_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "28c203a2-e385-4598-bdac-980dde4d91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # First linear layer expands the dimension (e.g., 768 → 3072)\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        # Second linear layer projects it back (e.g., 3072 → 768)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        # Non-linear activation\n",
    "        self.gelu = nn.GELU()\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, hidden_size]\n",
    "        x = self.linear_1(x)   # [batch, seq, intermediate_size]\n",
    "        x = self.gelu(x)       # non-linear transform\n",
    "        x = self.linear_2(x)   # [batch, seq, hidden_size]\n",
    "        x = self.dropout(x)    # apply dropout\n",
    "        return x               # same shape as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d43ca618-38e1-473c-93a6-d4e228ce53e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "feed_forward = FeedForward(config)\n",
    "ff_outputs = feed_forward(attention_output)\n",
    "print(ff_outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04890ae-818c-4ad8-a8b3-9fabf1982014",
   "metadata": {},
   "source": [
    "# Layer Normalization, Position Embeddings, Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "521ea841-985b-4543-bbcf-e61bb2f6d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include position embeddings, layer norm and dropout \n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Look up vectors for token IDs\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        # Learnable embeddings for positions [0..max_position_embeddings-1]\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: [batch_size, seq_len]\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "\n",
    "        # 1) Token embeddings: [B, T, H]\n",
    "        token_embeds = self.token_embeddings(input_ids)\n",
    "\n",
    "        # 2) Position ids: [0, 1, ..., T-1] for each example in batch\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        position_ids = position_ids.expand(batch_size, seq_len)  # [B, T]\n",
    "        position_embeds = self.position_embeddings(position_ids)  # [B, T, H]\n",
    "\n",
    "        # 3) Sum token + position, then normalize + dropout\n",
    "        embeddings = token_embeds + position_embeds          # [B, T, H]\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd430ffe-b1c1-495d-961d-70007a84d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include layer norm before the self attention, and also before the feed forward \n",
    "# Add a residual after the self attention and after the feed forward\n",
    "# LayerNorm → Multi-head attention → Residual\n",
    "# LayerNorm → FeedForward → Residual\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(config)  # your code\n",
    "        self.feed_forward = FeedForward(config)      # your code\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        # hidden_state: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # 1) Self-attention block with residual\n",
    "        attn_input = self.ln1(hidden_state)\n",
    "        attn_output = self.self_attn(attn_input)            # [B, T, H]\n",
    "        hidden_state = hidden_state + self.dropout(attn_output)\n",
    "\n",
    "        # 2) Feed-forward block with residual\n",
    "        ff_input = self.ln2(hidden_state)\n",
    "        ff_output = self.feed_forward(ff_input)             # [B, T, H]\n",
    "        hidden_state = hidden_state + self.dropout(ff_output)\n",
    "\n",
    "        return hidden_state   # [B, T, H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fa92958b-0466-4e92-a786-71e6f017d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassificationHead(nn.Module):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        # encoder_outputs: [batch_size, seq_len, hidden_size]\n",
    "        # Convention: use the first token's representation (position 0)\n",
    "        cls_embeds = encoder_outputs[:, 0, :]          # [batch_size, hidden_size]\n",
    "        x = self.dropout(cls_embeds)\n",
    "        logits = self.classifier(x)                    # [batch_size, num_labels]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b9398a9-32b7-4017-9590-2a23df39bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformerForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.encoder = EncoderLayer(config)               # single layer for demo\n",
    "        self.classifier = SequenceClassificationHead(config, num_labels)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # 1) ids → embeddings (token + position)\n",
    "        x = self.embeddings(input_ids)                    # [B, T, H]\n",
    "        # 2) embeddings → encoder layer (MHA + FFN + norms + residual)\n",
    "        x = self.encoder(x)                               # [B, T, H]\n",
    "        # 3) encoder output → logits\n",
    "        logits = self.classifier(x)                       # [B, num_labels]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "62072791-fe12-4819-a8f0-1418a55227a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0749, -2.0433]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9576, 0.0424]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model = MiniTransformerForSequenceClassification(config, num_labels=2)\n",
    "\n",
    "text = \"Artificial Intelligence is a game changer. Join the AI Career Launchpad program.\"\n",
    "inputs = tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]          # [1, seq_len]\n",
    "\n",
    "logits = model(input_ids)                # [1, 2]\n",
    "print(logits)\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "# If the classifier uses one output (instead of two), we can use sigmoid to convert into probability\n",
    "# Softmax ensures the probability for both the tasks add up to one \n",
    "# In BERT-style models with two outputs, we use softmax, not sigmoid.\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c65f31f-fb9e-48ee-807a-9de968be6a03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
